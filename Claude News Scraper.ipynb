{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sa7sZPshTHP3",
        "outputId": "a2af1a1c-b093-4cf7-f275-b8cec05cfd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.1)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.17.0)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=457237a1329698d69e5af056d8047e850655e0afa8b31ee4102fdfa6e90c2849\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=5ded3e11e416396fee47b579cbce29ccf49b6beeeab9dcde4e253214e674a6dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=348730eec06bec7f7e568b31490688a392db0d973f530a9bd3872f4daa999170\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=d8926236c4fe02fad685edafdc625a3fcd89008e2222d32001af7d391b68c909\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, PyDispatcher, jieba3k, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, feedparser, cssselect, constantly, automat, Twisted, requests-file, parsel, feedfinder2, tldextract, service-identity, itemloaders, scrapy, newspaper3k\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jieba3k-0.35.1 jmespath-1.0.1 newspaper3k-0.2.8 parsel-1.10.0 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy requests beautifulsoup4 newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCFuGcSZrWOs",
        "outputId": "0816f8e8-19d8-4513-e48a-bb3ab252e34a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyYxNot8re0I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMaxvTafTpUE",
        "outputId": "0360f8d2-4654-4f19-f12d-e2393ab2f951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yXOBFAv6aPen",
        "outputId": "ddeb7afa-066a-4d77-bdfb-78abbc4c14e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.47.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Downloading anthropic-0.47.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.47.1\n"
          ]
        }
      ],
      "source": [
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4SnKWJxS8e1"
      },
      "outputs": [],
      "source": [
        "from newspaper import Article\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import anthropic\n",
        "import json\n",
        "from typing import Dict, List, Optional\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Scrapes the article from the web\n",
        "def scrape_article(url):\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return {\n",
        "        'title': article.title,\n",
        "        'text': article.text,\n",
        "        'publish_date': article.publish_date,\n",
        "        'url': url\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P955KUBTEFP",
        "outputId": "fcb2c55c-05c2-4f8b-d2ea-7417f5c11965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'South Korean Lawmakers Impeach Acting President as Crisis Deepens',\n",
              " 'text': 'South Korea’s leadership crisis deepened on Friday after lawmakers voted to oust a second head of state, the acting president, in less than two weeks.\\n\\nThe move prolonged the political vacuum that has gripped South Korea since President Yoon Suk Yeol shocked the country this month by briefly putting it under military rule for the first time in decades.\\n\\nLawmakers impeached and suspended Mr. Yoon on Dec. 14 over the martial law move, and Prime Minister Han Duck-soo stepped in as acting president. But Mr. Han’s tenure would also prove short-lived, as opposition lawmakers voted on Friday to impeach Mr. Han, as well.\\n\\nThis was the first time South Korea had impeached an interim leader. It meant that South Korea continued to be without a strong elected leader who could take charge of the government and military in one of Washington’s most important allies, at a time when the country is grappling with North Korea’s nuclear threats and economic challenges at home. The political uncertainty has pushed business and consumer confidence lower and caused the currency, the won, to plunge.',\n",
              " 'publish_date': datetime.datetime(2024, 12, 26, 0, 0),\n",
              " 'url': 'https://www.nytimes.com/2024/12/26/world/asia/south-korea-impeach-president-han.html'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scrape_article(\"https://www.nytimes.com/2024/12/26/world/asia/south-korea-impeach-president-han.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOrvQCPWZ3-x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLkL8p01WPBq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Uses NYT API to get the top articles of the day\n",
        "def get_top_stories(section, api_key):\n",
        "    url = f\"https://api.nytimes.com/svc/topstories/v2/{section}.json\"\n",
        "    params = {'api-key': api_key}\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['results']\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycAqZlYZWTG4"
      },
      "outputs": [],
      "source": [
        "def get_all_articles():\n",
        "    api_key = input(\"Your news api key here:\")\n",
        "    sections = ['world', 'technology', 'science', 'health', 'business']\n",
        "\n",
        "    all_articles = {}\n",
        "    for section in sections:\n",
        "        articles = get_top_stories(section, api_key)\n",
        "        if articles:\n",
        "            all_articles[section] = [{\n",
        "                'title': article['title'],\n",
        "                'url': article['url'],\n",
        "                'published_date': article['published_date'],\n",
        "                'abstract': article['abstract'],\n",
        "                'section':section\n",
        "            } for article in articles]\n",
        "\n",
        "    return all_articles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j0v5kP8kW8eR"
      },
      "outputs": [],
      "source": [
        "all_articles = get_all_articles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX2-QZGwaGKQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NewsAnalyzer:\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"\n",
        "        Initialize the NewsAnalyzer with Claude API credentials.\n",
        "        \"\"\"\n",
        "        self.client = anthropic.Client(api_key=api_key)\n",
        "        self.model = \"claude-3-5-sonnet-20241022\"\n",
        "\n",
        "    def _extract_key_themes(self, articles: List[Dict]) -> List[str]:\n",
        "        \"\"\"Extract main themes from a list of articles using Claude.\"\"\"\n",
        "        # Prepare prompt for Claude\n",
        "        articles_text = \"\\n\".join([\n",
        "            f\"Title: {article.get('title', '')}\\n\"\n",
        "            f\"Abstract: {article.get('abstract', '')}\\n\"\n",
        "            for article in articles  # Analyze all\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Analyze these news articles and identify 3-4 key one-word themes. First describe each item in bullet points succinctly, then focus on major trends and patterns:\n",
        "\n",
        "        {articles_text}\n",
        "\n",
        "        Please provide the themes in a bullet-point format.\"\"\"\n",
        "\n",
        "        response = self.client.messages.create(\n",
        "            model=self.model,\n",
        "            max_tokens=300,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        # Extract themes from Claude's response\n",
        "        themes = response.content[0].text.split(\"\\n\")\n",
        "        return [theme.strip(\"• \").strip() for theme in themes if theme.strip()]\n",
        "\n",
        "    def analyze_news_data(self, news_data: Dict[str, List[Dict]]) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze news data and generate insights using Claude API.\n",
        "\n",
        "        Args:\n",
        "            news_data (Dict[str, List[Dict]]): Dictionary with sections as keys and lists of articles as values\n",
        "                Each article should have 'title', 'abstract', 'published_date', and 'url' fields\n",
        "\n",
        "        Returns:\n",
        "            Dict: Analysis results including:\n",
        "                - Article counts per section\n",
        "                - Key themes per section\n",
        "                - Recent headlines\n",
        "                - Timeline analysis\n",
        "        \"\"\"\n",
        "        analysis = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"sections\": {}\n",
        "        }\n",
        "\n",
        "        for section, articles in news_data.items():\n",
        "            if not articles:\n",
        "                continue\n",
        "\n",
        "            # Basic statistics\n",
        "            section_analysis = {\n",
        "                \"article_count\": len(articles),\n",
        "                \"recent_headlines\": [\n",
        "                    article[\"title\"] for article in articles  # most recent 10\n",
        "                ],\n",
        "                \"key_themes\": self._extract_key_themes(articles),\n",
        "                \"date_range\": {\n",
        "                    \"start\": min(article[\"published_date\"] for article in articles if article.get(\"published_date\")),\n",
        "                    \"end\": max(article[\"published_date\"] for article in articles if article.get(\"published_date\"))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Add to main analysis\n",
        "            analysis[\"sections\"][section] = section_analysis\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def generate_summary_report(self, analysis: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Generate a human-readable summary report from the analysis as if you are an expert on world news from Bloomberg. You are keen to spot any new confliencts, risks and opportunities\n",
        "        for current countries and companies. You are also keen to spot new supply and demand changes based on the headlines and themes. You also give critical advice to entrepreneurs. Cite\n",
        "        specific cases to support your advice, do not provide any generic ones.\n",
        "\n",
        "        Args:\n",
        "            analysis (Dict): Analysis output from analyze_news_data()\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted summary report\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Based on this news analysis data, create a concise summary report highlighting the most important insights:\n",
        "\n",
        "        {json.dumps(analysis, indent=2)}\n",
        "\n",
        "        Format the report with sections for:\n",
        "        1. Overall Coverage Summary\n",
        "        2. Key Themes by Section\n",
        "        3. Notable Recent Headlines\n",
        "        4. Your expert reflection of the current world conditions based on today's news.\n",
        "        5. Hidden trendes and opportunties for companies.\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.messages.create(\n",
        "            model=self.model,\n",
        "            max_tokens=1000,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ_mb8I9dUkU"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "def run():\n",
        "    # Initialize analyzer\n",
        "    analyzer = NewsAnalyzer(api_key=input(\"your claude key here:\"))\n",
        "\n",
        "    # Sample news data\n",
        "    news_data = all_articles\n",
        "\n",
        "    # Run analysis\n",
        "    analysis = analyzer.analyze_news_data(news_data)\n",
        "\n",
        "    # Generate report\n",
        "    report = analyzer.generate_summary_report(analysis)\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1fEcDmHePEU",
        "outputId": "358b2a3e-23b4-4a02-a45a-4e2e24123132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEWS ANALYSIS REPORT\n",
            "Date: February 24, 2025\n",
            "\n",
            "1. OVERALL COVERAGE SUMMARY\n",
            "Total articles analyzed: 151 across 5 major sections\n",
            "- World: 36 articles (24%)\n",
            "- Business: 34 articles (22%)\n",
            "- Technology: 29 articles (19%)\n",
            "- Science: 26 articles (17%)\n",
            "- Health: 26 articles (17%)\n",
            "\n",
            "2. KEY THEMES BY SECTION\n",
            "World:\n",
            "- Global power realignment (US-Russia-China dynamics)\n",
            "- European political transitions\n",
            "- Ongoing conflicts (Ukraine, Gaza)\n",
            "\n",
            "Technology:\n",
            "- AI industry power struggles\n",
            "- Regulatory pressures on tech giants\n",
            "- Cryptocurrency market volatility\n",
            "- Corporate restructuring\n",
            "\n",
            "Science/Health:\n",
            "- Government agency disruptions\n",
            "- Public health challenges\n",
            "- Research funding concerns\n",
            "- Scientific advancement despite obstacles\n",
            "\n",
            "Business:\n",
            "- Media industry transformation\n",
            "- Energy sector transitions\n",
            "- Corporate leadership changes\n",
            "- Market uncertainty\n",
            "\n",
            "3. NOTABLE RECENT HEADLINES\n",
            "Most Significant:\n",
            "- \"Three Years Into War in Ukraine, Trump Ushers in New World for Putin\"\n",
            "- \"OpenAI Uncovers Evidence of A.I.-Powered Chinese Surveillance Tool\"\n",
            "- \"Russia Is Wooing Western Energy Companies, but Will They Return?\"\n",
            "- \"Apple Vows to Build A.I. Servers in Houston and Spend $500 Billion in U.S.\"\n",
            "\n",
            "4. EXPERT REFLECTION\n",
            "The news landscape reveals a world in significant transition, characterized by:\n",
            "- Shifting global power dynamics with increasing China-Russia cooperation\n",
            "- Democratic institutions under pressure in Western nations\n",
            "- Technological disruption accelerating across all sectors\n",
            "- Public health and scientific institutions facing structural challenges\n",
            "- Business environment marked by uncertainty and rapid change\n",
            "\n",
            "5. HIDDEN TRENDS AND OPPORTUNITIES\n",
            "For Companies:\n",
            "1. Domestic Manufacturing: Growing opportunities in US-based tech manufacturing\n",
            "2. AI Integration: Increasing demand for AI solutions across traditional industries\n",
            "3. Energy Transition: Opening markets in alternative energy as geopolitical tensions affect traditional sources\n",
            "4. Healthcare Tech: Growing space for private sector solutions amid public sector disruption\n",
            "5. Security Services: Rising demand for cybersecurity and surveillance solutions\n",
            "\n",
            "Emerging Opportunities:\n",
            "- Localized supply chain solutions\n",
            "- AI-human hybrid services\n",
            "- Clean energy infrastructure\n",
            "- Healthcare technology innovation\n",
            "- Digital security solutions\n",
            "\n",
            "The analysis suggests companies should focus on adaptability, domestic capabilities, and technological integration while preparing for continued global instability and regulatory changes.\n"
          ]
        }
      ],
      "source": [
        "run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaONQ54GKY9I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
